{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 영어 문장에서의 차원 축소\n",
    "\n",
    "## 주제\n",
    "\n",
    "차원 축소 기법들에 대해 간략하게 설명한 후,\n",
    "자연어 처리 기법을 통해 영어 문장을 받아들이고 분석한 후 특성을 추출해 데이터화한 뒤,\n",
    "이를 기반으로 간단한 알고리즘만을 사용하였을 때와 여러 다양한 차원 축소 기법을 동원하였을 때의 각 단어가 문장 전체의 의미에 미치는 영향 추출 성능 및 효율성 비교\n",
    "\n",
    "최종적으로, 이러한 기법들을 동원하여 문장을 입력하면 해당 문장을 분석하여 각 단어의 색상(또는 배경 색상)을 해당 단어가 문장의 의미에 미치는 영향 수준에 맟게 칠하는 프로그램 작성.\n",
    "\n",
    "## 구현 과정\n",
    "\n",
    "1. 자연어 처리 부분은 nltk에서 기본으로 제공하는 함수들을 이용하고, 간단한 word2id 알고리즘을 구현하여 sparse matrix를 제작.\n",
    "2. 필요에 따라 차원 축소 방안을 선택적으로 적용할 수 있도록 구현.\n",
    "3. 차원 축소를 사용하지 않을 때와 서로 다른 차원 축소 방안을 사용할 때, 여러 개의 차원 축소 방안을 중첩하여 사용할 때의 결과의 변화와 성능 상의 차이를 탐구.\n",
    "4. 위 과정에서 구현한 코드를 재사용하여, 주제에서 언급한 프로그램 제작. Python의 tkinter를 사용할 예정. 문장 입력 시 실시간으로 하이라이트해 주는 기능 추가.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\eric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\eric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\eric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\eric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 5670400ns to process.\n",
      "Took 5433700ns to highlight.\n",
      "Took 159225900ns to draw Word Cloud.\n",
      "Took 7888300ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1056400ns to highlight.\n",
      "Took 160918700ns to draw Word Cloud.\n",
      "Took 88074300ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 2116600ns to highlight.\n",
      "Took 154287100ns to draw Word Cloud.\n",
      "Took 45199700ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1320300ns to highlight.\n",
      "Took 139347200ns to draw Word Cloud.\n",
      "Took 33361400ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1325400ns to highlight.\n",
      "Took 139010900ns to draw Word Cloud.\n",
      "Took 44353200ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1274800ns to highlight.\n",
      "Took 141827100ns to draw Word Cloud.\n",
      "Took 50184500ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1695400ns to highlight.\n",
      "Took 144728300ns to draw Word Cloud.\n",
      "Took 34387500ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1380500ns to highlight.\n",
      "Took 140223200ns to draw Word Cloud.\n",
      "Took 68426600ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1172200ns to highlight.\n",
      "Took 137129700ns to draw Word Cloud.\n",
      "Took 63601900ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1352300ns to highlight.\n",
      "Took 136673700ns to draw Word Cloud.\n",
      "Took 55615200ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1469000ns to highlight.\n",
      "Took 141885900ns to draw Word Cloud.\n",
      "Took 50589800ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1371100ns to highlight.\n",
      "Took 139914500ns to draw Word Cloud.\n",
      "Took 46607400ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1370400ns to highlight.\n",
      "Took 137808600ns to draw Word Cloud.\n",
      "Took 36351800ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1219500ns to highlight.\n",
      "Took 147027100ns to draw Word Cloud.\n",
      "Took 63484800ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 2102300ns to highlight.\n",
      "Took 145876400ns to draw Word Cloud.\n",
      "Took 35525100ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1288100ns to highlight.\n",
      "Took 143177800ns to draw Word Cloud.\n",
      "Took 25929300ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1339800ns to highlight.\n",
      "Took 138870000ns to draw Word Cloud.\n",
      "Took 49632100ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1280900ns to highlight.\n",
      "Took 145221200ns to draw Word Cloud.\n",
      "Took 63774000ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1638200ns to highlight.\n",
      "Took 137952800ns to draw Word Cloud.\n",
      "Took 40602000ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1700800ns to highlight.\n",
      "Took 145852200ns to draw Word Cloud.\n",
      "Took 37745500ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1147100ns to highlight.\n",
      "Took 140915400ns to draw Word Cloud.\n",
      "Took 34746400ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1145400ns to highlight.\n",
      "Took 143999600ns to draw Word Cloud.\n",
      "Took 40341300ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1154100ns to highlight.\n",
      "Took 144027000ns to draw Word Cloud.\n",
      "Took 32756400ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1631900ns to highlight.\n",
      "Took 138209200ns to draw Word Cloud.\n",
      "Took 49695400ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1504400ns to highlight.\n",
      "Took 144355800ns to draw Word Cloud.\n",
      "Took 46192200ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 2016600ns to highlight.\n",
      "Took 155793100ns to draw Word Cloud.\n",
      "Took 59989100ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 2184300ns to highlight.\n",
      "Took 165683800ns to draw Word Cloud.\n",
      "Took 10070600ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1660200ns to highlight.\n",
      "Took 139411600ns to draw Word Cloud.\n",
      "Took 9988700ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1515700ns to highlight.\n",
      "Took 141010800ns to draw Word Cloud.\n",
      "Took 9037300ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1510300ns to highlight.\n",
      "Took 142797800ns to draw Word Cloud.\n",
      "Took 13268400ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1926500ns to highlight.\n",
      "Took 138191800ns to draw Word Cloud.\n",
      "Took 10402700ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1740400ns to highlight.\n",
      "Took 133760000ns to draw Word Cloud.\n",
      "Took 8822000ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 2000700ns to highlight.\n",
      "Took 141121400ns to draw Word Cloud.\n",
      "Took 10812500ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 2417400ns to highlight.\n",
      "Took 139753800ns to draw Word Cloud.\n",
      "Took 16035900ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 2270500ns to highlight.\n",
      "Took 136695700ns to draw Word Cloud.\n",
      "Took 9944300ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1660800ns to highlight.\n",
      "Took 132953200ns to draw Word Cloud.\n",
      "Took 6680300ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1524900ns to highlight.\n",
      "Took 141371800ns to draw Word Cloud.\n",
      "Took 6012700ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1544600ns to highlight.\n",
      "Took 146020800ns to draw Word Cloud.\n",
      "Took 6398300ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1770200ns to highlight.\n",
      "Took 143899100ns to draw Word Cloud.\n",
      "Took 5216200ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1296500ns to highlight.\n",
      "Took 143804600ns to draw Word Cloud.\n",
      "Took 4502700ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1326800ns to highlight.\n",
      "Took 139149900ns to draw Word Cloud.\n",
      "Took 6015800ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1545900ns to highlight.\n",
      "Took 147276200ns to draw Word Cloud.\n",
      "Took 4200100ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1358700ns to highlight.\n",
      "Took 154219000ns to draw Word Cloud.\n",
      "Took 5378600ns to process.\n",
      "MSE: 0.00001177342676315504\n",
      "Took 2510800ns to highlight.\n",
      "Took 136322700ns to draw Word Cloud.\n",
      "Took 5551900ns to process.\n",
      "MSE: 0.00002913246450580628\n",
      "Took 3753700ns to highlight.\n",
      "Took 140099400ns to draw Word Cloud.\n",
      "Took 2703500ns to process.\n",
      "MSE: 0.00004256766645950797\n",
      "Took 2143400ns to highlight.\n",
      "Took 143154900ns to draw Word Cloud.\n",
      "Took 5871100ns to process.\n",
      "MSE: 0.00015714203584650440\n",
      "Took 2806500ns to highlight.\n",
      "Took 141199000ns to draw Word Cloud.\n",
      "Took 3578200ns to process.\n",
      "MSE: 0.00016823627440240772\n",
      "Took 2233500ns to highlight.\n",
      "Took 145483100ns to draw Word Cloud.\n",
      "Took 2857800ns to process.\n",
      "MSE: 0.00009923841616256929\n",
      "Took 2426400ns to highlight.\n",
      "Took 143053200ns to draw Word Cloud.\n",
      "Took 2754600ns to process.\n",
      "MSE: 0.00012910988877525600\n",
      "Took 1818400ns to highlight.\n",
      "Took 150148300ns to draw Word Cloud.\n",
      "Took 2798900ns to process.\n",
      "MSE: 0.00009647839767763605\n",
      "Took 1885100ns to highlight.\n",
      "Took 143175200ns to draw Word Cloud.\n",
      "Took 2629100ns to process.\n",
      "MSE: 0.00003935053213866072\n",
      "Took 1739000ns to highlight.\n",
      "Took 141802500ns to draw Word Cloud.\n",
      "Took 2784400ns to process.\n",
      "MSE: 0.00006795761259287633\n",
      "Took 1815900ns to highlight.\n",
      "Took 143890100ns to draw Word Cloud.\n",
      "Took 3066600ns to process.\n",
      "MSE: 0.00007018535610478460\n",
      "Took 1776600ns to highlight.\n",
      "Took 136849600ns to draw Word Cloud.\n",
      "Took 2806100ns to process.\n",
      "MSE: 0.00000993192363259083\n",
      "Took 1655200ns to highlight.\n",
      "Took 144214900ns to draw Word Cloud.\n",
      "Took 2829300ns to process.\n",
      "MSE: 0.00015908960066180792\n",
      "Took 1856000ns to highlight.\n",
      "Took 141575800ns to draw Word Cloud.\n",
      "Took 3096600ns to process.\n",
      "MSE: 0.00038472830632146741\n",
      "Took 1892100ns to highlight.\n",
      "Took 138887000ns to draw Word Cloud.\n",
      "Took 2675200ns to process.\n",
      "MSE: 0.00016444055456632512\n",
      "Took 2106800ns to highlight.\n",
      "Took 139817900ns to draw Word Cloud.\n",
      "Took 3060700ns to process.\n",
      "MSE: 0.00010162307497589591\n",
      "Took 1744200ns to highlight.\n",
      "Took 136599400ns to draw Word Cloud.\n",
      "Took 2490300ns to process.\n",
      "MSE: 0.00002816074898605345\n",
      "Took 1906500ns to highlight.\n",
      "Took 157640000ns to draw Word Cloud.\n",
      "Took 2992500ns to process.\n",
      "MSE: 0.00001679043105895434\n",
      "Took 2020900ns to highlight.\n",
      "Took 152990300ns to draw Word Cloud.\n",
      "Took 22799600ns to process.\n",
      "MSE: 0.00002974929041608928\n",
      "Took 2849400ns to highlight.\n",
      "Took 136174400ns to draw Word Cloud.\n",
      "Took 62852300ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1484600ns to highlight.\n",
      "Took 142501700ns to draw Word Cloud.\n",
      "Took 38228100ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1213600ns to highlight.\n",
      "Took 140971400ns to draw Word Cloud.\n",
      "Took 56179200ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1174300ns to highlight.\n",
      "Took 138380200ns to draw Word Cloud.\n",
      "Took 34034000ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1295500ns to highlight.\n",
      "Took 133595500ns to draw Word Cloud.\n",
      "Took 61600100ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1336700ns to highlight.\n",
      "Took 141666700ns to draw Word Cloud.\n",
      "Took 45847400ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1964300ns to highlight.\n",
      "Took 148234100ns to draw Word Cloud.\n",
      "Took 39412900ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1177100ns to highlight.\n",
      "Took 139727300ns to draw Word Cloud.\n",
      "Took 55684600ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1181600ns to highlight.\n",
      "Took 136078600ns to draw Word Cloud.\n",
      "Took 47462500ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1244200ns to highlight.\n",
      "Took 135714100ns to draw Word Cloud.\n",
      "Took 44436900ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1348800ns to highlight.\n",
      "Took 135491000ns to draw Word Cloud.\n",
      "Took 53323700ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 2068300ns to highlight.\n",
      "Took 140342200ns to draw Word Cloud.\n",
      "Took 64693000ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1266900ns to highlight.\n",
      "Took 141134900ns to draw Word Cloud.\n",
      "Took 40778900ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1435000ns to highlight.\n",
      "Took 145387500ns to draw Word Cloud.\n",
      "Took 33778900ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1282000ns to highlight.\n",
      "Took 141288000ns to draw Word Cloud.\n",
      "Took 38215800ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1149100ns to highlight.\n",
      "Took 139836800ns to draw Word Cloud.\n",
      "Took 42430700ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1351100ns to highlight.\n",
      "Took 142699000ns to draw Word Cloud.\n",
      "Took 44208600ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1308000ns to highlight.\n",
      "Took 144096200ns to draw Word Cloud.\n",
      "Took 48240700ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1372900ns to highlight.\n",
      "Took 152626100ns to draw Word Cloud.\n",
      "Took 13726100ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1164700ns to highlight.\n",
      "Took 140775100ns to draw Word Cloud.\n",
      "Took 51728900ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1163200ns to highlight.\n",
      "Took 152985400ns to draw Word Cloud.\n",
      "Took 26856700ns to process.\n",
      "MSE: 0.00000000000000000000\n",
      "Took 1303000ns to highlight.\n",
      "Took 140302000ns to draw Word Cloud.\n",
      "Took 1975364800ns to process.\n",
      "MSE: 3.67723527674834604895\n",
      "Took 2700100ns to highlight.\n",
      "Took 133739900ns to draw Word Cloud.\n",
      "Took 1904340800ns to process.\n",
      "MSE: 0.00002470305441105615\n",
      "Took 2805000ns to highlight.\n",
      "Took 129975900ns to draw Word Cloud.\n",
      "Took 1923344000ns to process.\n",
      "MSE: 0.00002023282107503031\n",
      "Took 2690400ns to highlight.\n",
      "Took 134849900ns to draw Word Cloud.\n",
      "Took 1908200900ns to process.\n",
      "MSE: 0.00003151415204794586\n",
      "Took 2576800ns to highlight.\n",
      "Took 130509000ns to draw Word Cloud.\n",
      "Took 1893729100ns to process.\n",
      "MSE: 0.00004176956489784396\n",
      "Took 2558000ns to highlight.\n",
      "Took 130655600ns to draw Word Cloud.\n",
      "Took 2102293000ns to process.\n",
      "MSE: 0.00003126306955904327\n",
      "Took 2667500ns to highlight.\n",
      "Took 136198300ns to draw Word Cloud.\n",
      "Took 1928014400ns to process.\n",
      "MSE: 0.00007076710587300337\n",
      "Took 2522800ns to highlight.\n",
      "Took 123130600ns to draw Word Cloud.\n",
      "Took 1927575800ns to process.\n",
      "MSE: 0.00007203000301019409\n",
      "Took 2621200ns to highlight.\n",
      "Took 124323900ns to draw Word Cloud.\n",
      "Took 1921529400ns to process.\n",
      "MSE: 0.00002101902005578522\n",
      "Took 3310500ns to highlight.\n",
      "Took 137869400ns to draw Word Cloud.\n",
      "Took 1930374700ns to process.\n",
      "MSE: 0.00002423181984322156\n",
      "Took 2638600ns to highlight.\n",
      "Took 342236900ns to draw Word Cloud.\n",
      "Took 1893936300ns to process.\n",
      "MSE: 0.00004512554130899532\n",
      "Took 2543800ns to highlight.\n",
      "Took 136872500ns to draw Word Cloud.\n",
      "Took 1951737600ns to process.\n",
      "MSE: 0.00007783941647899147\n",
      "Took 2732700ns to highlight.\n",
      "Took 143238800ns to draw Word Cloud.\n",
      "Took 3550875900ns to process.\n",
      "MSE: 0.00002831370985308887\n",
      "Took 2984400ns to highlight.\n",
      "Took 129441300ns to draw Word Cloud.\n",
      "Took 2338726900ns to process.\n",
      "MSE: 0.00000362980290479929\n",
      "Took 2671500ns to highlight.\n",
      "Took 116479800ns to draw Word Cloud.\n",
      "Took 2082969600ns to process.\n",
      "MSE: 0.00001216074402002040\n",
      "Took 2538900ns to highlight.\n",
      "Took 117962900ns to draw Word Cloud.\n",
      "Took 2105241700ns to process.\n",
      "MSE: 0.00001112864958776396\n",
      "Took 2526300ns to highlight.\n",
      "Took 118611900ns to draw Word Cloud.\n",
      "Took 2111513000ns to process.\n",
      "MSE: 0.00000599373983159869\n",
      "Took 2613700ns to highlight.\n",
      "Took 130583300ns to draw Word Cloud.\n",
      "Took 2161620700ns to process.\n",
      "MSE: 0.00000589253649529566\n",
      "Took 2626000ns to highlight.\n",
      "Took 119497900ns to draw Word Cloud.\n",
      "Took 2128513300ns to process.\n",
      "MSE: 0.00000801724583163447\n",
      "Took 2683200ns to highlight.\n",
      "Took 117841700ns to draw Word Cloud.\n",
      "Took 2323155000ns to process.\n",
      "MSE: 0.00000854362758423455\n",
      "Took 2681500ns to highlight.\n",
      "Took 120117400ns to draw Word Cloud.\n",
      "Took 2137427600ns to process.\n",
      "MSE: 0.00000446240037202501\n",
      "Took 2652300ns to highlight.\n",
      "Took 113297500ns to draw Word Cloud.\n",
      "Took 2153919200ns to process.\n",
      "MSE: 0.00000799776820196535\n",
      "Took 3123300ns to highlight.\n",
      "Took 122373800ns to draw Word Cloud.\n",
      "Took 2131883500ns to process.\n",
      "MSE: 0.00000819109751999233\n",
      "Took 2582600ns to highlight.\n",
      "Took 121049300ns to draw Word Cloud.\n",
      "Took 2156429700ns to process.\n",
      "MSE: 0.00000578034001426099\n",
      "Took 2657300ns to highlight.\n",
      "Took 114451700ns to draw Word Cloud.\n",
      "Took 3331522400ns to process.\n",
      "MSE: 0.00241295903761779961\n",
      "Took 2662500ns to highlight.\n",
      "Took 154102200ns to draw Word Cloud.\n",
      "Took 2781764800ns to process.\n",
      "MSE: 0.00011664443668911231\n",
      "Took 2600800ns to highlight.\n",
      "Took 148525500ns to draw Word Cloud.\n",
      "Took 2370252300ns to process.\n",
      "MSE: 0.00018885707422168672\n",
      "Took 2646800ns to highlight.\n",
      "Took 141413000ns to draw Word Cloud.\n",
      "Took 2326276400ns to process.\n",
      "MSE: 0.00012287139159914751\n",
      "Took 2689300ns to highlight.\n",
      "Took 142042100ns to draw Word Cloud.\n",
      "Took 2333510100ns to process.\n",
      "MSE: 0.00010212153975148989\n",
      "Took 2513400ns to highlight.\n",
      "Took 140437200ns to draw Word Cloud.\n",
      "Took 2374243000ns to process.\n",
      "MSE: 0.00013689724438489242\n",
      "Took 2546500ns to highlight.\n",
      "Took 137769700ns to draw Word Cloud.\n",
      "Took 2545595100ns to process.\n",
      "MSE: 0.00016693353338846219\n",
      "Took 2487600ns to highlight.\n",
      "Took 141401400ns to draw Word Cloud.\n",
      "Took 2327847700ns to process.\n",
      "MSE: 0.00014203343163675801\n",
      "Took 2607800ns to highlight.\n",
      "Took 142739600ns to draw Word Cloud.\n",
      "Took 2337888700ns to process.\n",
      "MSE: 0.00023919682952632128\n",
      "Took 2920100ns to highlight.\n",
      "Took 141209400ns to draw Word Cloud.\n",
      "Took 2324070800ns to process.\n",
      "MSE: 0.00007888226673505589\n",
      "Took 2603400ns to highlight.\n",
      "Took 149604000ns to draw Word Cloud.\n",
      "Took 2386071600ns to process.\n",
      "MSE: 0.00015075140204446358\n",
      "Took 2623600ns to highlight.\n",
      "Took 150400600ns to draw Word Cloud.\n",
      "Took 2595012800ns to process.\n",
      "MSE: 0.00011574134244567627\n",
      "Took 2585400ns to highlight.\n",
      "Took 141682900ns to draw Word Cloud.\n",
      "Took 2335254600ns to process.\n",
      "MSE: 0.00012848794428918885\n",
      "Took 2577000ns to highlight.\n",
      "Took 147889000ns to draw Word Cloud.\n",
      "Took 2333550600ns to process.\n",
      "MSE: 0.00011911591585329753\n",
      "Took 2568500ns to highlight.\n",
      "Took 146256900ns to draw Word Cloud.\n",
      "Took 2353507300ns to process.\n",
      "MSE: 0.00010988215736513663\n",
      "Took 2596800ns to highlight.\n",
      "Took 150031700ns to draw Word Cloud.\n",
      "Took 2357401000ns to process.\n",
      "MSE: 0.00009862568463857482\n",
      "Took 3951700ns to highlight.\n",
      "Took 144000400ns to draw Word Cloud.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import colorsys\n",
    "import cv2\n",
    "import wordcloud\n",
    "from PIL import Image, ImageTk\n",
    "from tkinter import *\n",
    "import tkinter\n",
    "import tkinter.ttk\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, SimpleRNN, LSTM\n",
    "\n",
    "# Download required NLTK DLCs\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize stop-words\n",
    "stopwords=set(stopwords.words('english'))\n",
    "\n",
    "# Prepare the data using TensorFlow\n",
    "def prepare_tf(corpus: list[str]):\n",
    "    # Drop stop-words from the corpus\n",
    "    def preprocess_text(text):\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        return [word for word in tokens if word not in stopwords]\n",
    "    corpus = [' '.join(preprocess_text(doc)) for doc in corpus]\n",
    "\n",
    "    # Tokenize and create sequences (using TensorFlow Tokenizer)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    sequences = tokenizer.texts_to_sequences(corpus)\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    # Padding sequences to ensure equal length\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    return pad_sequences(sequences, maxlen=max_length),len(sequences),word_index\n",
    "\n",
    "\n",
    "# Prepare the data using NLTK\n",
    "def prepare_nltk(corpus: list[str],window_size:int=2)->tuple[list[str],dict[str,int],np.ndarray]:\n",
    "    # Tokenize corpus (using nltk word_tokenize)\n",
    "    tokenized_corpus = [word_tokenize(doc.lower()) for doc in corpus]\n",
    "\n",
    "    # Enumerate volabularies in the corpus and map to the int\n",
    "    vocab = set(word for doc in tokenized_corpus for word in doc if word not in stopwords)\n",
    "    vocab2idx = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "    return tokenized_corpus,vocab2idx,build_co_matrix(tokenized_corpus,vocab2idx,window_size)\n",
    "\n",
    "\n",
    "# Build up the co-occurrence matrix from the corpus and word2index mapping\n",
    "def build_co_matrix(tokenized_corpus: list[str], word2idx: dict[str,int], window_size: int = 2)->np.ndarray:\n",
    "    co = np.zeros((len(word2idx), len(word2idx)))\n",
    "    for corpus in tokenized_corpus:\n",
    "        for i, word in enumerate(corpus):\n",
    "            # Skip stop-words\n",
    "            if word in stopwords:\n",
    "                continue\n",
    "            \n",
    "            # Scan [i-ws, i+ws] to create co-occurrence matrix\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(corpus), i + window_size + 1)\n",
    "            for j in range(start, end):\n",
    "                if i != j and corpus[j] in word2idx:\n",
    "                    co[word2idx[word], word2idx[corpus[j]]] += 1\n",
    "    return co\n",
    "\n",
    "\n",
    "# PPMI(Positive Pairwise Mutual Information)\n",
    "def ppmi(co_matrix: np.ndarray)->np.ndarray:\n",
    "    total_sum = np.sum(co_matrix)\n",
    "    word_sums = np.sum(co_matrix, axis=0)\n",
    "    ppmi_matrix = np.maximum(np.log((co_matrix * total_sum) / ((word_sums[:, None] * word_sums[None, :]) + 1e-8) + 1e-8), 0)\n",
    "    return ppmi_matrix\n",
    "\n",
    "\n",
    "# Analyze the input sentence using specified embeddings to calculate the weight of each words(vocabularies) in the sentence.\n",
    "def analyze(sentence:str, embeddings:np.ndarray, word2idx:dict[str,int]):\n",
    "    # Tokenize the input sentence\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "\n",
    "    # Calculate mean for all words(vocabularies) in the input sentence from embeddings\n",
    "    sentence_vector = np.mean([embeddings[word2idx[word]] for word in tokens if word in word2idx], axis=0)\n",
    "\n",
    "    # Calculate weight for each word(vocabulary) in the input sentence\n",
    "    weights = {}\n",
    "    for word in tokens:\n",
    "        if word in word2idx:\n",
    "            word_vector = embeddings[word2idx[word]]\n",
    "            weight = np.dot(word_vector, sentence_vector)\n",
    "            weights[word] = weight\n",
    "    return weights\n",
    "\n",
    "\n",
    "# Dimension Reduction using the sklearn.decomposition.* classes (which support 'fit_transform' and 'inverse_transform')\n",
    "def dr_with_sklearn(cls, embeddings: np.ndarray)->np.ndarray:\n",
    "    scaler = StandardScaler()\n",
    "    embeddings = scaler.fit_transform(embeddings)\n",
    "    embeddings = cls.fit_transform(embeddings)\n",
    "    embeddings = cls.inverse_transform(embeddings)\n",
    "    embeddings = scaler.inverse_transform(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Dimension Reduction using SVD (Singular Value Decomposition)\n",
    "def dr_with_svd(embeddings: np.ndarray, n_components:int=2)->np.ndarray:\n",
    "    scaler = StandardScaler()\n",
    "    embeddings = scaler.fit_transform(embeddings)\n",
    "    _,_,Vt=np.linalg.svd(embeddings) # SVD (We can safely drop U and E because what we need is Vt)\n",
    "    embeddings=np.dot(embeddings,Vt[:n_components,:].T) # Inverse SVD\n",
    "    embeddings=np.dot(embeddings,Vt[:n_components,:])\n",
    "    embeddings = scaler.inverse_transform(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Feature Extraction and Embedding using CNN\n",
    "def dr_with_cnn(tokenized_data, seqcount, word2idx, embedding_dim=50):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(word2idx) + 1, output_dim=embedding_dim))\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return dr_with_model(model, tokenized_data, seqcount)\n",
    "\n",
    "\n",
    "# Feature Extraction and Embedding using Simple RNN\n",
    "def dr_with_rnn(tokenized_data, seqcount, word2idx, embedding_dim=50):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(word2idx) + 1, output_dim=embedding_dim))\n",
    "    model.add(SimpleRNN(50, return_sequences=False))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return dr_with_model(model, tokenized_data, seqcount)\n",
    "\n",
    "\n",
    "# Feature Extraction and Embedding using LSTM\n",
    "def dr_with_lstm(tokenized_data, seqcount, word2idx, embedding_dim=50):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(word2idx) + 1, output_dim=embedding_dim))\n",
    "    model.add(LSTM(50, return_sequences=False))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return dr_with_model(model, tokenized_data, seqcount)\n",
    "\n",
    "\n",
    "# Feature Extraction and Embedding using TensorFlow sequential model\n",
    "def dr_with_model(model, tokenized_data, seqcount):\n",
    "    # Compile and train the model\n",
    "    labels = np.array([1 for _ in range(seqcount)])  # Dummy labels for training\n",
    "    model.fit(tokenized_data, labels, epochs=60, verbose=0)\n",
    "\n",
    "    # Extract word embeddings\n",
    "    return model.layers[0].get_weights()[0]\n",
    "\n",
    "\n",
    "def process(corpus: str, mode: str, options: dict):\n",
    "    nltk_mode=['pca','svd','tsvd','ipca']\n",
    "    tf_mode=['cnn','rnn','lstm']\n",
    "    mode=mode.lower()\n",
    "    if mode in nltk_mode:\n",
    "        # Prepare for NLTK\n",
    "        tokenized_corpus,word2idx,embeddings = prepare_nltk(corpus,options.get('window_size',2))\n",
    "\n",
    "        # Apply PPMI\n",
    "        if options.get('ppmi',True):\n",
    "            embeddings=ppmi(embeddings)\n",
    "\n",
    "        # Perform dimension reduction\n",
    "        n_components=options.get('n_components',2)\n",
    "        if mode=='pca':\n",
    "            embeddings=dr_with_sklearn(PCA(n_components=n_components), embeddings)\n",
    "        elif mode=='svd':\n",
    "            embeddings=dr_with_svd(embeddings, n_components)\n",
    "        elif mode=='tsvd':\n",
    "            embeddings=dr_with_sklearn(TruncatedSVD(n_components=n_components), embeddings)\n",
    "        elif mode=='ipca':\n",
    "            embeddings=dr_with_sklearn(IncrementalPCA(n_components=n_components), embeddings)\n",
    "    elif mode in tf_mode:\n",
    "        tokenized_data,seqcount,word2idx=prepare_tf(corpus)\n",
    "        embedding_dim=options.get('embedding_dim',50)\n",
    "        if mode=='cnn':\n",
    "            embeddings=dr_with_cnn(tokenized_data,seqcount,word2idx,embedding_dim)\n",
    "        elif mode=='rnn':\n",
    "            embeddings=dr_with_rnn(tokenized_data,seqcount,word2idx,embedding_dim)\n",
    "        elif mode=='lstm':\n",
    "            embeddings=dr_with_lstm(tokenized_data,seqcount,word2idx,embedding_dim)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown mode: {mode}')\n",
    "\n",
    "    return analyze(' '.join(corpus), embeddings, word2idx)\n",
    "\n",
    "tk=Tk()\n",
    "tk.title('Word Effect Calculator')\n",
    "\n",
    "modebox=tkinter.ttk.Combobox(tk, height=15, values=['PCA','SVD','TSVD','IPCA','CNN','RNN','LSTM'])\n",
    "modebox.pack(anchor='n',fill='both')\n",
    "modebox.set('SVD')\n",
    "\n",
    "para_input=Text(tk)\n",
    "para_input.pack(anchor='n',expand=True,fill='both')\n",
    "\n",
    "wc_canvas=Canvas(tk,width=600,height=400)\n",
    "wc_canvas.pack(anchor='center', fill='both')\n",
    "canvas_img = None #wc_canvas.create_image(0,0,anchor='nw')\n",
    "\n",
    "undirty=False\n",
    "prev_result=None\n",
    "\n",
    "def mse(a1,a2):\n",
    "    s1=set(a1.keys())\n",
    "    s2=set(a2.keys())\n",
    "    sxor=s1.symmetric_difference(s2)\n",
    "    s=0\n",
    "    for w in sxor:\n",
    "        if w in a1:\n",
    "            s += a1[w]**2\n",
    "        else:\n",
    "            s += a2[w]**2\n",
    "    for w in s1.intersection(s2):\n",
    "        s += (a1[w]-a2[w])**2\n",
    "    return s/len(a1)\n",
    "\n",
    "\n",
    "def text_updated():\n",
    "    global undirty\n",
    "    global canvas_img\n",
    "    global prev_result\n",
    "    if undirty:\n",
    "        return\n",
    "    \n",
    "    undirty=True\n",
    "    try:\n",
    "        tk.call(para_input, 'edit', 'modified', 0)\n",
    "    finally:\n",
    "        undirty=False\n",
    "\n",
    "    text=para_input.get(1.0,END)\n",
    "    if len(text)<10:\n",
    "        return\n",
    "    t=time.perf_counter_ns()\n",
    "    result=process(text.split('. '), modebox.get(), {})\n",
    "    print(f'Took {(time.perf_counter_ns()-t)}ns to process.')\n",
    "    if prev_result != None:\n",
    "        print(f'MSE: {mse(result, prev_result):.20f}')\n",
    "    prev_result=result\n",
    "    min_w=999999\n",
    "    max_w=0\n",
    "    for word in result:\n",
    "        weight=result[word]\n",
    "        min_w=min(min_w,weight)\n",
    "        max_w=max(max_w,weight)\n",
    "        \n",
    "    t=time.perf_counter_ns()\n",
    "    sortd=list(result.items())\n",
    "    sortd.sort(key=lambda x: len(x[0]))\n",
    "    text=' '+text\n",
    "    for word, weight in sortd:\n",
    "        if min_w<max_w:\n",
    "            weight_normalized=(weight-min_w)/(max_w-min_w)\n",
    "        else:\n",
    "            weight_normalized=weight\n",
    "        idx=0\n",
    "        while True:\n",
    "            found=text.find(f' {word} ',idx)\n",
    "            if found==-1:\n",
    "                break\n",
    "            #print(f'Highlight added for {found} to {found+len(word)} for word {word} with weight {weight_normalized}')\n",
    "            idx=found+len(word)\n",
    "            para_input.tag_add(word, f'1.0 + {found} chars', f'1.0 + {found+len(word)} chars')\n",
    "\n",
    "        color_hex='#%02x%02x%02x' % tuple(map(lambda x: int(x*255),colorsys.hsv_to_rgb(weight_normalized, max(0.4,weight_normalized), 1.0)))\n",
    "        \n",
    "        para_input.tag_config(word, background=color_hex)\n",
    "    print(f'Took {(time.perf_counter_ns()-t)}ns to highlight.')\n",
    "\n",
    "    t=time.perf_counter_ns()\n",
    "    wc=wordcloud.WordCloud(background_color='white', width=600, height=400)\n",
    "    wc.generate_from_frequencies(result)\n",
    "    wc_canvas.image_=ImageTk.PhotoImage(wc.to_image())\n",
    "    \n",
    "    if not canvas_img:\n",
    "        canvas_img = wc_canvas.create_image(0,0,anchor='nw', image=wc_canvas.image_)\n",
    "    else:\n",
    "        wc_canvas.itemconfig(canvas_img, image=wc_canvas.image_)\n",
    "    print(f'Took {(time.perf_counter_ns()-t)}ns to draw Word Cloud.')\n",
    "    \n",
    "    \n",
    "para_input.bind_all('<<Modified>>', lambda event: text_updated())\n",
    "para_input.bind_all('<F5>', lambda event: text_updated())\n",
    "modebox.bind_all('<<ComboboxSelected>>', lambda event: text_updated())\n",
    "\n",
    "#but=Button(tk,text='Update',command=text_updated)\n",
    "#but.pack()\n",
    "tk.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
